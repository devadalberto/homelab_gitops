{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab GitOps \u2014 Uranus","text":"<p>Welcome to the documentation portal for the Uranus homelab GitOps stack. The site is generated with MkDocs Material and captures the automation, platform layout, and operational practices that keep the environment reproducible.</p>"},{"location":"#day-1-quickstart","title":"Day-1 Quickstart","text":"<pre><code>git clone https://github.com/devadalberto/homelab_gitops.git\ncd homelab_gitops\ncp .env.example .env\n# Edit pfSense bridges, LAN settings, and point PF_SERIAL_INSTALLER_PATH at the Netgate installer\nmake up\n</code></pre> <p><code>.env.example</code> now highlights only the required pfSense settings (VM name, WAN mode, bridges, installer path, and LAN ranges) so you can focus on the zero-touch workflow. Optional variables for ingress hosts, applications, and chart versions can still live in a private <code>.env</code>.</p> <p>The <code>make up</code> target walks the staged automation targets in order\u2014<code>doctor</code>, <code>net.ensure</code>, <code>pf.preflight</code>, <code>pf.config</code>, <code>pf.ztp</code>, <code>k8s.bootstrap</code>, and <code>status</code>\u2014to validate host dependencies, render pfSense assets, provision the VM, and stand up Kubernetes with a single command.\u3010F:Makefile\u2020L8-L63\u3011</p> <p>If you chose <code>br0</code>, the host will reboot once, then resume automatically: - pfSense VM defined with the <code>pfSense_config</code> ISO attached so first boot auto-imports <code>/opt/homelab/pfsense/config/config.xml</code>. - <code>make up</code> orchestrates Minikube, MetalLB, Traefik, cert-manager, Postgres, backups, AWX, Observability, Django, and Flux end-to-end.</p> <p>A deeper walk-through of every subsystem, bootstrap dependency, and GitOps controller lives in the Reference guide. That page also includes Mermaid sequence/state diagrams that are rendered as part of the documentation build.</p>"},{"location":"#platform-highlights","title":"Platform Highlights","text":"<ul> <li>Network &amp; Edge \u2014 pfSense CE provides DHCP/DNS/NAT for the homelab VLAN, MetalLB advertises the <code>10.10.0.0/24</code> LoadBalancer range, and Traefik terminates TLS with certificates issued by cert-manager.</li> <li>Cluster Runtime \u2014 Minikube forms the Kubernetes control plane while Flux continuously reconciles manifests and Helm releases stored in this repository.</li> <li>Stateful Services \u2014 Bitnami PostgreSQL runs in the <code>data</code> namespace with nightly WAL backups pushed to the hostPath defined in <code>.env</code>.</li> <li>User-Facing Applications \u2014 AWX, the Django multiproject demo, and the kube-prometheus-stack observability suite are delivered through the same GitOps pipeline.</li> </ul>"},{"location":"#where-to-go-next","title":"Where to Go Next","text":"<ul> <li>Follow the Workflow guide for GitOps reconciliation details, automation entry points, and documentation tooling expectations.</li> <li>Consult the Troubleshooting runbooks when bootstrap or day-2 operations need deeper investigation.</li> <li>Use the Reference for credentials management, pfSense refresh procedures, backup policies, and the project changelog.</li> </ul>"},{"location":"reference/","title":"Reference","text":"<p>This guide aggregates the architecture, operational procedures, and historical notes for the Uranus homelab GitOps stack. Pair it with the Workflow overview for reconciliation details and automation entry points.</p>"},{"location":"reference/#platform-overview","title":"Platform Overview","text":"<p>The environment stitches together on-premises virtualization, Kubernetes tooling, and GitOps pipelines so that every environment can be reproduced deterministically. The following diagram highlights the primary subsystems and the traffic flow between them.</p> <p></p>"},{"location":"reference/#network-edge","title":"Network &amp; Edge","text":"<ul> <li>pfSense CE supplies DHCP, DNS, and outbound NAT for the homelab VLAN. Firewall port-forward examples remain disabled by default so operators can opt in to each exposure after validating policies.</li> <li>MetalLB advertises LoadBalancer services within the <code>10.10.0.0/24</code> range published by pfSense. The values are templated via environment variables inside <code>.env</code> to keep addresses portable between labs.</li> <li>Traefik terminates TLS using certificates issued by the internal cert-manager hierarchy and provides a default ingress class for workloads.</li> </ul>"},{"location":"reference/#cluster-runtime","title":"Cluster Runtime","text":"<ul> <li>Minikube forms the base Kubernetes control plane. Bootstrap scripts configure container runtimes, storage classes, and hostPath directories aligned with the repository layout.</li> <li>Flux reconciles Helm releases and Kubernetes manifests stored in this Git repository, ensuring the cluster continuously converges towards the desired state.</li> <li>Cert-Manager provisions a root Certificate Authority and application leaf certs. The root CA export procedure lives in Internal CA.</li> </ul>"},{"location":"reference/#stateful-services","title":"Stateful Services","text":"<ul> <li>PostgreSQL (Bitnami chart) runs in the <code>data</code> namespace. A nightly CronJob pushes WAL backups to the hostPath defined by <code>LABZ_MOUNT_BACKUPS</code>.</li> <li>Persistent hostPath volumes for application storage are parameterized through <code>.env</code> to align with local disk layout.</li> </ul>"},{"location":"reference/#user-facing-applications","title":"User-Facing Applications","text":"<ul> <li>AWX provides Ansible automation inside the <code>awx</code> namespace with persistent volumes and TLS termination handled by Traefik.</li> <li>Homepage publishes the default landing page for the homelab at <code>https://home.lab-minikube.labz.home.arpa/</code>, loading its layout from <code>k8s/apps/homepage/configmap.yaml</code> and pulling widget credentials (<code>openweathermap_api_key</code>, <code>uptime_kuma_api_key</code>) from the SOPS-encrypted secret at <code>apps/homepage/sops-secrets/homepage-secrets.yaml</code>.</li> <li>Django Multiproject Demo showcases the platform deployment pipeline, including container image preloading via <code>apps/django-multiproject/load-image.sh</code>.</li> <li>Observability stack is powered by <code>kube-prometheus-stack</code>, exposing Grafana, Prometheus, and Alertmanager through Traefik-managed Ingresses.</li> </ul>"},{"location":"reference/#secrets-and-credentials","title":"Secrets and Credentials","text":"<ul> <li>SOPS/AGE secret placeholders live in the repo (<code>.sops/</code>). Actual encrypted files should be stored separately and decrypted only on trusted hosts.</li> <li>Export <code>SOPS_AGE_KEY_FILE</code> (for example, <code>export SOPS_AGE_KEY_FILE=\"$PWD/.sops/age.key\"</code>) before invoking <code>sops</code> locally or running Flux bootstrap scripts so the controllers mount the same private key during reconciliation.</li> <li>Traefik ingress routes always reference TLS secrets; the repo defaults to the internal CA but can be swapped for ACME with external DNS integration.</li> </ul>"},{"location":"reference/#manage-encrypted-application-secrets","title":"Manage Encrypted Application Secrets","text":"<p>The AWX admin, Postgres superuser, and Pi-hole admin Kubernetes Secrets live in this repository as SOPS manifests and are encrypted for the Age recipient defined in <code>.sops/.sops.yaml</code>. To edit any of these secrets:</p> <ol> <li>Export the matching Age private key so SOPS can decrypt locally (for example <code>export SOPS_AGE_KEY_FILE=$HOME/.config/sops/age/keys.txt</code>).</li> <li>Open the manifest with SOPS (<code>sops awx/sops-secrets/awx-admin.sops.yaml</code>, <code>sops data/postgres/sops-secrets/postgres-superuser.yaml</code>, or <code>sops apps/pihole/sops-secrets/admin-secret.yaml</code>). SOPS handles the decrypt/edit/re-encrypt cycle automatically when the editor closes.</li> <li>Apply the updated manifest back to the cluster (<code>kubectl apply -f &lt;path-to-secret&gt;</code>).</li> </ol>"},{"location":"reference/#rotate-awx-admin-credentials","title":"Rotate AWX Admin Credentials","text":"<p>The AWX operator expects the <code>awx-admin</code> secret to exist before the instance comes online. When rotating the admin password, generate a new credential, update the <code>stringData.password</code> field in <code>awx/sops-secrets/awx-admin.sops.yaml</code>, apply the manifest, and then synchronize the running AWX instance:</p> <pre><code>kubectl -n awx exec deployment/awx-task -- awx-manage changepassword admin '&lt;new-password&gt;'\n</code></pre> <p>Bounce the AWX pods if the operator does not reconcile automatically.</p>"},{"location":"reference/#rotate-postgres-superuser-credentials","title":"Rotate Postgres Superuser Credentials","text":"<p>The bootstrap Postgres chart consumes <code>data/postgres/sops-secrets/postgres-superuser.yaml</code> for the <code>pg-superuser</code> secret. When changing the password, ensure the <code>stringData.postgres-password</code> field and the <code>stringData.database-url</code> connection string stay in sync before applying the manifest.</p>"},{"location":"reference/#rotate-pi-hole-admin-password","title":"Rotate Pi-hole Admin Password","text":"<p>The Pi-hole Helm release references the SOPS-encrypted manifest at <code>apps/pihole/sops-secrets/admin-secret.yaml</code> to supply the <code>pihole-admin</code> Secret before the chart installs. To change the web UI password:</p> <ol> <li>Decrypt and edit the manifest:    <pre><code>sops apps/pihole/sops-secrets/admin-secret.yaml\n</code></pre></li> <li>Replace the value under <code>stringData.password</code> with a new credential and save; SOPS will re-encrypt on exit.</li> <li>Apply the updated secret and restart the release so the deployment consumes the new password:    <pre><code>kubectl apply -f apps/pihole/sops-secrets/admin-secret.yaml\nkubectl -n pihole rollout restart deployment/pihole\n</code></pre></li> </ol>"},{"location":"reference/#homepage-api-keys","title":"Homepage API Keys","text":"<p>The Homepage dashboard reads its widget credentials from <code>apps/homepage/sops-secrets/homepage-secrets.yaml</code>. The secret must define <code>openweathermap_api_key</code> for the weather widget and <code>uptime_kuma_api_key</code> for the Uptime Kuma status widget referenced in <code>k8s/apps/homepage/configmap.yaml</code>.</p> <ol> <li>Open the manifest with SOPS:    <pre><code>sops apps/homepage/sops-secrets/homepage-secrets.yaml\n</code></pre></li> <li>Update the API keys under the <code>secrets.yaml</code> document.</li> <li>Save and exit so SOPS re-encrypts the file, then commit the change or apply it manually (<code>kubectl apply -f apps/homepage/sops-secrets/homepage-secrets.yaml</code>).</li> </ol>"},{"location":"reference/#manage-nextcloud-credentials","title":"Manage Nextcloud Credentials","text":"<p>Flux deploys Nextcloud with the Bitnami chart using three SOPS-encrypted secrets stored under <code>apps/nextcloud/sops-secrets/</code>:</p> <ul> <li><code>nextcloud-admin</code> maps to the Helm values <code>nextcloudUsername</code>, <code>nextcloudPassword</code>, and <code>nextcloudEmail</code> for the initial UI administrator account.\u3010F:apps/nextcloud/sops-secrets/admin-secret.yaml\u2020L1-L24\u3011\u3010F:k8s/apps/nextcloud/helmrelease.yaml\u2020L55-L71\u3011</li> <li><code>nextcloud-redis</code> provides the external cache password consumed by <code>externalCache.password</code>.\u3010F:apps/nextcloud/sops-secrets/redis-secret.yaml\u2020L1-L18\u3011\u3010F:k8s/apps/nextcloud/helmrelease.yaml\u2020L72-L76\u3011</li> <li><code>nextcloud-database</code> carries the PostgreSQL DSN plus discrete host, port, database, username, and password values that feed the chart's <code>externalDatabase</code> block and populate the <code>DATABASE_URL</code> environment variable exposed to the pod.\u3010F:apps/nextcloud/sops-secrets/database-secret.yaml\u2020L1-L24\u3011\u3010F:k8s/apps/nextcloud/helmrelease.yaml\u2020L77-L104\u3011</li> </ul> <p>Update the credentials by decrypting the manifests with <code>sops</code>, editing the <code>stringData</code> fields, and applying the files back to the cluster:</p> <pre><code>sops apps/nextcloud/sops-secrets/admin-secret.yaml\nsops apps/nextcloud/sops-secrets/redis-secret.yaml\nsops apps/nextcloud/sops-secrets/database-secret.yaml\nkubectl apply -f apps/nextcloud/sops-secrets/\n</code></pre> <p>Keep the <code>dsn</code> string synchronized with the individual host/port/username/password entries so the HelmRelease renders consistent values. Adjust the ingress host, TLS mapping, and upload limit in <code>k8s/apps/nextcloud/helmrelease.yaml</code> if your lab uses a different FQDN or quota than the defaults committed to Git.\u3010F:k8s/apps/nextcloud/helmrelease.yaml\u2020L25-L54\u3011</p>"},{"location":"reference/#grafana-admin-credential-management","title":"Grafana Admin Credential Management","text":"<p>Grafana is deployed through the <code>kube-prometheus-stack</code> chart. The admin credentials are stored in the SOPS-encrypted manifest at <code>observability/sops-secrets/grafana-admin.yaml</code>.</p>"},{"location":"reference/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install SOPS on your workstation.</li> <li>Import the Homelab SOPS GPG key (<code>06D8D7726588CB103AE9717413BF4A9EA7B7C3F8</code>) from the team credential vault into your local keyring. This key is not stored in the repository; request access through the normal secure channel if it is not already present.</li> </ol>"},{"location":"reference/#retrieve-the-password","title":"Retrieve the Password","text":"<pre><code># Decrypt the manifest and print the admin credentials\nsops -d observability/sops-secrets/grafana-admin.yaml \\\n  | yq '.stringData[\"admin-user\"] + \":\" + .stringData[\"admin-password\"]' -r\n</code></pre>"},{"location":"reference/#rotate-the-password","title":"Rotate the Password","text":"<ol> <li>Open the manifest with <code>sops</code>:    <pre><code>sops observability/sops-secrets/grafana-admin.yaml\n</code></pre></li> <li>Replace the value under <code>stringData.admin-password</code> with a new random password. The helper below generates a 32-character value:    <pre><code>python - &lt;&lt;'PY'\nimport secrets, string\nalphabet = string.ascii_letters + string.digits + '!@#$%^&amp;*()-_=+'\nprint(''.join(secrets.choice(alphabet) for _ in range(32)))\nPY\n</code></pre></li> <li>Save and exit; <code>sops</code> will re-encrypt the document automatically.</li> <li>Commit the updated secret and let Flux reconcile, or apply it manually if you need the change immediately:    <pre><code>kubectl apply -f observability/sops-secrets/grafana-admin.yaml\n</code></pre></li> </ol>"},{"location":"reference/#why-this-matters","title":"Why This Matters","text":"<p>The chart values file (<code>observability/kps-values.yaml</code>) points Grafana at the <code>grafana-admin</code> Secret via <code>grafana.admin.existingSecret</code>. This keeps credentials out of Git history while still allowing GitOps workflows to manage the rendered Secret manifest.</p>"},{"location":"reference/#internal-ca","title":"Internal CA","text":"<p>Export the root CA to trust on clients:</p> <pre><code>kubectl -n cert-manager get secret labz-root-ca-secret -o jsonpath='{.data.ca\\.crt}' | base64 -d &gt; labz-root-ca.crt\n</code></pre>"},{"location":"reference/#pfsense-and-networking","title":"pfSense and Networking","text":""},{"location":"reference/#refreshing-pfsense-bootstrap-media","title":"Refreshing pfSense Bootstrap Media","text":"<p>pfSense reads configuration overrides from the secondary CD-ROM labelled <code>pfSense_config</code> during its first boot. The ISO lives at <code>${WORK_ROOT}/pfsense/config/pfSense-config.iso</code> (defaults to <code>/opt/homelab/pfsense/config/pfSense-config.iso</code>). When lab addressing or credentials change, regenerate and reattach the media before starting the VM:</p> <pre><code>./pfsense/pf-config-gen.sh   # rebuild config.xml and the pfSense_config ISO (requires genisoimage or mkisofs)\nvirsh shutdown ${VM_NAME}    # skip if the VM has never been started\nsudo ./scripts/pf-ztp.sh --env-file ./.env\n</code></pre> <p><code>pf-ztp.sh</code> swaps both the installer disk and config ISO, inserting media live when possible. If the VM is running and libvirt refuses a hot change, stop it first or run <code>virsh change-media ${VM_NAME} sdy ${WORK_ROOT}/pfsense/config/pfSense-config.iso --insert --force --config</code> manually after the shutdown completes. The helper re-reads <code>.env</code> on each invocation, so confirm <code>PF_INSTALLER_SRC</code> still references the installer download before triggering a refresh. Legacy <code>PF_SERIAL_INSTALLER_PATH</code>/<code>PF_ISO_PATH</code> entries are still honored when present (<code>PF_ISO_PATH</code> remains the VGA build toggle).</p>"},{"location":"reference/#enabling-nat-examples","title":"Enabling NAT Examples","text":"<p>pfSense GUI \u2192 Firewall \u2192 NAT \u2192 Port Forward \u2192 edit example \u2192 uncheck Disable \u2192 Save \u2192 Apply. Then navigate to Firewall \u2192 Rules \u2192 WAN \u2192 enable the matching pass rule if present.</p>"},{"location":"reference/#configuration-reference","title":"Configuration Reference","text":""},{"location":"reference/#environment-variables-and-mapping","title":"Environment Variables and Mapping","text":"Existing var Canonical var Used by <code>LAB_DOMAIN_BASE</code> <code>LABZ_DOMAIN</code> Ingress/hosts <code>LAB_CLUSTER_SUB</code> (keep as-is) Cluster FQDNs (misc) <code>METALLB_POOL_START/END</code> <code>LABZ_METALLB_RANGE</code> MetalLB AddressPool <code>TRAEFIK_LOCAL_IP</code> (derive from VIP) Docs only <code>PG_BACKUP_HOSTPATH</code> <code>LABZ_MOUNT_BACKUPS</code> Backups <code>/srv/*</code> mounts <code>LABZ_MOUNT_*</code> hostPath PVs"},{"location":"reference/#jellyfin-configuration","title":"Jellyfin configuration","text":"<p>The Jellyfin overlay sources runtime values from the <code>jellyfin-config</code> ConfigMap and the SOPS-managed <code>jellyfin-api</code> Secret. Update <code>k8s/apps/jellyfin/configmap.yaml</code> when the timezone, published URL, or media mount path change, and edit <code>k8s/apps/jellyfin/sops-secrets/jellyfin-api.yaml</code> with the Age key to rotate the API token exposed as <code>JELLYFIN_API_KEY</code> in the deployment.\u3010F:k8s/apps/jellyfin/configmap.yaml\u2020L1-L9\u3011\u3010F:k8s/apps/jellyfin/sops-secrets/jellyfin-api.yaml\u2020L1-L29\u3011\u3010F:k8s/apps/jellyfin/deployment.yaml\u2020L1-L42\u3011</p> <p>After updating the MetalLB fields in <code>.env</code>, regenerate the Flux manifest so the GitOps path and bootstrap helpers agree on the pool range:</p> <pre><code>./scripts/render_metallb_pool_manifest.sh --env-file ./.env\n</code></pre>"},{"location":"reference/#traefik-dashboard","title":"Traefik Dashboard","text":"<p>The Traefik dashboard is published at https://traefik.labz.home.arpa/dashboard/ behind the <code>websecure</code> entry point. The route terminates TLS with the <code>traefik-dashboard-tls</code> certificate issued by <code>labz-ca-issuer</code>, so install the internal root CA on any workstation before browsing.</p> <p>Basic authentication protects the dashboard. Use the following credentials when prompted:</p> Username Password <code>ops</code> <code>xOps!Traefik2024</code>"},{"location":"reference/#backup-disaster-recovery","title":"Backup &amp; Disaster Recovery","text":"<ul> <li>PostgreSQL backups are scheduled by the CronJob in <code>data/postgres/backup-cron.yaml</code>.</li> <li>HostPath directories are prepared during the Kubernetes bootstrap helper (<code>scripts/k8s-up.sh</code>), which ensures application data paths exist before workloads deploy.\u3010F:scripts/k8s-up.sh\u2020L625-L704\u3011</li> <li>To restore, re-run the bootstrap scripts (<code>make up</code>) and re-apply the saved secrets using Flux or manual <code>kubectl</code> commands.</li> </ul>"},{"location":"reference/#extending-the-platform","title":"Extending the Platform","text":"<ul> <li>Add new apps by creating namespaces in <code>apps/</code> and referencing them inside the main <code>Makefile</code> or Flux <code>kustomizations</code>.</li> <li>Leverage the documentation pipeline described in Workflow to record decisions, diagrams, and operational runbooks.</li> <li>Use the GitHub Pages deployment to surface the latest diagrams and Markdown whenever changes merge into <code>main</code>.</li> </ul>"},{"location":"reference/#changelog","title":"Changelog","text":""},{"location":"reference/#2025-09-20t000000000000z","title":"2025-09-20T00:00:00.000000Z","text":"<ul> <li>Extended the GitOps CI workflow with a cached lint job that runs ShellCheck, yamllint, and kubeconform before manifest validation.</li> <li>Documented how to invoke the lint suite locally via <code>make fmt</code>/<code>pre-commit run --all-files</code> so contributors can mirror the CI checks.</li> </ul>"},{"location":"reference/#2025-09-19t000000000000z","title":"2025-09-19T00:00:00.000000Z","text":"<ul> <li>Documented a host-only WireGuard workflow in the README, including package installation, key management, sample <code>wg0.conf</code>, router/VPS considerations, client configuration, and verification guidance for the <code>192.168.88.0/24</code> overlay.</li> <li>Linked troubleshooting content to the new remote access guidance so runbooks point operators at the same procedure.</li> </ul>"},{"location":"reference/#2025-09-18t000000000000z","title":"2025-09-18T00:00:00.000000Z","text":"<ul> <li>Recorded current release cadence and pinned the GitOps stack to the n-1 builds exercised in testing: Kubernetes v1.31.3, MetalLB 0.14.7, Traefik 27.0.2, cert-manager 1.16.3, Bitnami PostgreSQL 16.2.6, kube-prometheus-stack 65.5.0, and AWX operator 2.20.0.</li> <li>Updated the Flux CLI bootstrap helper to install v2.3.0 so local environments reconcile with the same binary verified in automation.</li> <li>Refreshed documentation to note the latest upstream versions alongside the pinned releases for easier future upgrades.</li> </ul>"},{"location":"reference/#2025-09-17t080000000000z","title":"2025-09-17T08:00:00.000000Z","text":"<ul> <li>Pin Flux-managed chart versions to the builds exercised in automation: MetalLB 0.14.5, cert-manager 1.15.3, and Traefik 26.0.0.</li> <li>Document the Helm chart upgrade workflow (bump <code>k8s/addons/*/release.yaml</code>, stage with <code>make k8s.bootstrap</code>, then update docs) to keep production in lockstep with staging.\u3010F:Makefile\u2020L46-L58\u3011</li> <li>Wire Flux <code>dependsOn</code> for Traefik so upgrades wait on MetalLB and cert-manager health, surfacing dependency issues earlier.</li> </ul>"},{"location":"reference/#2025-09-10t044738065634z","title":"2025-09-10T04:47:38.065634Z","text":"<ul> <li>Restructured repo to modular stages (Makefile).</li> <li>Added <code>bootstrap.sh</code> with br0 default + safe reboot/resume.</li> <li>pfSense CE 2.8.0 ISO URL; config.xml generator with labz domain + NAT examples (disabled).</li> <li>Minikube + MetalLB + Traefik + cert-manager internal CA.</li> <li>Postgres with 7-day nightly backups to hostPath.</li> <li>AWX (small), kube-prometheus-stack, Django multiproject app.</li> <li>Flux controllers installed (no remote Git).</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Use these runbooks to diagnose bootstrap hiccups and day-2 drift in the Uranus homelab environment.</p>"},{"location":"troubleshooting/#quick-triage-checklist","title":"Quick Triage Checklist","text":"<ul> <li>MetalLB not advertising? Ensure pfSense LAN is <code>10.10.0.0/24</code> and does not overlap with WAN ranges.</li> <li>Traefik returning <code>404</code>? Check the Ingress class <code>traefik</code> and confirm certificate secrets exist.</li> <li>AWX pending? Wait for operator rollout; check <code>kubectl -n awx get pods</code> for image pulls and migrations.</li> <li>Need remote access to troubleshoot without exposing services? Follow the WireGuard Remote Access (Host-Only) steps in the repository README.</li> </ul>"},{"location":"troubleshooting/#pfsense-zero-touch-provisioning","title":"pfSense Zero-Touch Provisioning","text":"<p>Keep pfSense automation reliable by validating the media, sequencing, and health checks described below.</p>"},{"location":"troubleshooting/#iso-image-production","title":"ISO Image Production","text":"<ul> <li>Always invoke <code>xorriso -as mkisofs</code> when rebuilding the pfSense bootstrap media so the image is hybrid and bootable by libvirt.</li> <li>Capture the full <code>xorriso</code> command in logs or the runbook to make regressions easy to spot; missing the flag silently produces an unusable image.</li> </ul>"},{"location":"troubleshooting/#automation-sequencing","title":"Automation Sequencing","text":"<ul> <li>Verify the VM is started before the ZTP script begins its wait loops; otherwise the script exhausts its retries while the domain is still powered off.</li> <li>When iterating on ZTP, include an explicit <code>virsh start</code> (or equivalent) step prior to invoking the automation to keep the timeline deterministic.</li> </ul>"},{"location":"troubleshooting/#bootstrap-health-checks","title":"Bootstrap Health Checks","text":"<ul> <li>Extend the wait budget to cover slow first boots and add a fallback probe to <code>192.168.1.1</code>\u2014the pfSense factory default\u2014so the automation can detect when it never pulled the custom config.</li> <li>Treat a successful <code>192.168.1.1</code> response as a cue to rebuild the USB image and re-run the provisioning workflow.</li> </ul>"},{"location":"troubleshooting/#nic-model-policy","title":"NIC Model Policy","text":"<ul> <li>Keep pfSense NICs on <code>virtio</code> by default; only switch models when explicitly required and the VM is shut down.</li> <li>Document any overrides (such as <code>PF_FORCE_E1000=true</code>) so that they are intentional and reversible.</li> </ul>"},{"location":"troubleshooting/#additional-notes","title":"Additional Notes","text":"<ul> <li>Keep <code>.env</code> aligned with virtio defaults (<code>PF_FORCE_E1000=false</code>) and the intended LAN link (<code>PF_LAN_LINK=bridge:pfsense-lan</code>).</li> <li>Generate the pfSense configuration (<code>pf-config-gen.sh</code>) before running ZTP so <code>/opt/homelab/pfsense/config/config.xml</code> exists.</li> </ul>"},{"location":"troubleshooting/#runbook-for-make-up-failures","title":"Runbook for <code>make up</code> Failures","text":"<ol> <li>Rebuild the USB image with <code>xorriso -as mkisofs</code> and confirm the command used.</li> <li>Ensure the pfSense domain is running: <code>sudo virsh list --name | grep pfsense-uranus</code> and start it if necessary.</li> <li>Capture current wiring: <code>sudo virsh domiflist pfsense-uranus</code> and <code>sudo virsh dumpxml pfsense-uranus</code> (filter <code>&lt;interface&gt;</code> sections).</li> <li>Inspect storage: <code>sudo virsh domblklist pfsense-uranus</code> to confirm USB and disk attachments.</li> <li>Check host bridges: <code>ip -br a | grep -E 'virbr|br0'</code> to ensure expected links exist.</li> <li>Probe pfSense reachability:</li> <li><code>ping -c1 -W1 10.10.0.1</code> and <code>curl -kIs --connect-timeout 5 https://10.10.0.1/</code> for the provisioned address.</li> <li><code>ping -c1 -W1 192.168.1.1</code> as the fallback factory-default probe.</li> <li>If only the fallback address responds, rebuild the USB media and repeat the ZTP run.</li> <li>Collect additional diagnostics with <code>sudo ./scripts/diag-pfsense.sh</code> if available.</li> </ol>"},{"location":"workflow/","title":"Workflow","text":"<p>This guide explains how changes move from a developer workstation into the Uranus homelab along with the tooling that keeps documentation and automation reproducible.</p>"},{"location":"workflow/#gitops-control-loop","title":"GitOps Control Loop","text":"<p>Flux controllers continuously reconcile the repository against the running cluster. The sequence diagram below summarizes the control flow whenever a manifest or Helm values file changes.</p> <p></p> <ol> <li>Developers edit Kubernetes manifests or Helm values and push to <code>main</code>.</li> <li>Flux's source-controller pulls the repository, verifying commit signatures if configured.</li> <li>The kustomize-controller and helm-controller render templates and compare them against the live cluster state.</li> <li>Drift is corrected by applying the resulting manifests through the Kubernetes API server.</li> <li>Events and reconciliation status are exported via Prometheus and surfaced inside Grafana dashboards.</li> </ol>"},{"location":"workflow/#application-automation","title":"Application Automation","text":"<p>Flux manages the PostgreSQL stack under <code>k8s/data/postgres/</code>. The co-located <code>HelmRepository</code> pulls the Bitnami index while the <code>HelmRelease</code> pins chart version <code>16.2.6</code>, loads overrides from <code>data/postgres/pg-values.yaml</code>, and installs the workload into the <code>data</code> namespace. Backups are reconciled by the same Kustomization via the hostPath <code>PersistentVolume</code>, <code>PersistentVolumeClaim</code>, and nightly <code>CronJob</code> that execute <code>pg_dump</code> into the mounted backup share.</p> <ul> <li>Tune retention or the destination directory by editing <code>data/postgres/backup-cron.yaml</code> and <code>data/postgres/backup-pv.yaml</code> before committing changes.</li> <li>Database superuser credentials live in <code>data/postgres/sops-secrets/postgres-superuser.yaml</code>. Update the manifest with <code>sops</code>, commit the change, and let Flux reconcile so the Helm chart picks up the new secret version.</li> </ul>"},{"location":"workflow/#documentation-workflow","title":"Documentation Workflow","text":"<p>The repository uses MkDocs with the Material theme to convert Markdown content and Mermaid diagrams into a browsable site. Contributors should mirror the tooling described below when authoring updates.</p>"},{"location":"workflow/#dependencies","title":"Dependencies","text":"Tool Purpose Installation Notes Python \u2265 3.9 MkDocs + plugins <code>pip install -r docs/requirements.txt</code> Node.js \u2265 18 Mermaid CLI <code>npm install -g @mermaid-js/mermaid-cli</code> (optional when using <code>npx</code>) Make Automation entry point Included on most Linux distros and macOS <p>Install the Python packages inside a virtual environment to avoid polluting system interpreters.</p>"},{"location":"workflow/#local-authoring","title":"Local Authoring","text":"<ul> <li>Edit Markdown under the <code>docs/</code> directory. The navigation menu is defined in <code>mkdocs.yml</code>.</li> <li>Place Mermaid sources next to the page they support (for example, <code>docs/diagrams/*.mmd</code>).</li> <li>Run <code>make docs-serve</code> while iterating. This regenerates all diagrams, starts <code>mkdocs serve</code>, and binds to <code>0.0.0.0:8000</code> for remote previews.</li> </ul>"},{"location":"workflow/#diagram-conventions","title":"Diagram Conventions","text":"<ul> <li>Output format defaults to SVG so diagrams remain crisp when zoomed inside the Material theme.</li> <li>Keep diagrams focused and reference them from the relevant pages using standard Markdown image syntax.</li> <li>When diagrams reference Kubernetes objects, stick to namespace/name notation (e.g., <code>awx/awx-operator</code>).</li> </ul>"},{"location":"workflow/#continuous-integration","title":"Continuous Integration","text":"<p>GitHub Actions execute the following steps on every push to <code>main</code>:</p> <ol> <li>Install MkDocs Material, the Mermaid plugin, and Mermaid CLI dependencies.</li> <li>Run <code>make docs</code> to regenerate all diagrams and ensure the MkDocs build succeeds with <code>--strict</code> mode.</li> <li>Deploy the rendered site to the <code>gh-pages</code> branch via <code>mkdocs gh-deploy</code>.</li> </ol> <p>Pull requests run through steps 1-2 to validate documentation without publishing.</p>"},{"location":"workflow/#pre-commit-hook","title":"Pre-commit Hook","text":"<p>Install <code>pre-commit</code> to mirror the repository linting locally:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Running <code>pre-commit run --all-files</code> executes shellcheck, shfmt, yamllint, markdownlint, and a guard that rejects libvirt domain XMLs containing <code>&lt;video&gt;</code> devices so headless guests are preserved.\u3010F:.pre-commit-config.yaml\u2020L2-L24\u3011\u3010F:scripts/check-libvirt-no-video.sh\u2020L1-L38\u3011 Re-run <code>make docs</code> or <code>make docs-serve</code> whenever content or diagrams change; those targets remain the supported path to regenerate the site during reviews.\u3010F:Makefile\u2020L77-L96\u3011</p>"}]}